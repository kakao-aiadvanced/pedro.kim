from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

def run(user_query, contexts, response):
    llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
    prompt_template = ChatPromptTemplate.from_messages([
        (
            "system",
            "You are a helpful AI assistant who compares a user query and a "
            "response to the query, checking if the response contains any "
            "false or nonsensical information. Your job will be given as "
            "follows:\n"
            "\n"
            "<userQuery>user query</userQuery>\n"
            "<contexts>contexts that were given along to generate the "
            "response</contexts>\n"
            "<response>response that was generated by another</response>\n"
            "\n"
            "Your answer must be in JSON format. If the response seems "
            "appropriate, say {{\"hallucination\": \"yes\"}}.  If not, say "
            "{{\"hallucination\": \"no\"}}."
        ),
        (
            "human",
            "<userQuery>{user_query}</userQuery>\n"
            "<contexts>{contexts}</userQuery>\n"
            "<response>{response}</response>\n"
        )
    ])
    chain = (
        prompt_template
        | llm
        | StrOutputParser()
    )
    return chain.invoke({
        "user_query": user_query,
        "contexts": "\n\n".join(doc.page_content for doc in contexts),
        "response": response
    })
